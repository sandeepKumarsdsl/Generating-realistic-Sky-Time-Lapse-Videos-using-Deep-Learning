**Title:** [Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks]

**Authors:** [Wei Xiong], [Wenhan Luo], [Lin Ma], [Wei Liu], [Jiebo Luo]

**Publication Venue:** [IEEE/Salt Lake City, UT, USA]

**Publication Year:** [2018]

**Summary:**
[The project presents a Multi-stage Dynamic Generative Adversarial Network (MD-GAN) for generating high-resolution time-lapse videos predicting future frames with realistic content and vivid motion dynamics, outperforming state-of-the-art models, and introducing the use of Gram matrices for motion modeling.]



**Title:** [DTVNet: Dynamic Time-lapse Video Generation via Single Still Image]

**Authors:** [Jiangning Zhang], [Chao Xu], [Liang Liu], [Mengmeng Wang], [Xia Wu], [Yong Liu], [Yunliang Jiang]

**Publication Venue:** [Computer Vision â€“ ECCV 2020]

**Publication Year:** [2020]

**Summary:**
[This project introduces DTVNet, an end-to-end dynamic time-lapse video generation framework, leveraging Optical Flow Encoder (OFE) and Dynamic Video Generator (DVG) submodules to produce diverse time-lapse videos from a single landscape image based on normalized motion vectors, achieving high-quality, dynamic, and varied video generation.]
